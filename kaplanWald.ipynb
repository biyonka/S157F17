{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Kaplan-Wald Confidence Bound for a Nonnegative Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section presents an approach to finding lower confidence bounds for the mean of a nonnegative random variable described in H.M. Kaplan's website, http://printmacroj.com/martMean.htm. That work fleshes out an idea due to Wald (1945, 2004), and is closely related to a technique presented in Kaplan (1987).\n",
    "For another derivation, see Stark & Teague (2014) https://www.usenix.org/system/files/jets/issues/0301/overview/jets_0301-stark.pdf\n",
    "\n",
    "\n",
    "We have an iid sequence of random variables $X_1, X_2, \\ldots$ such that $\\mathbb P \\{X_j \\ge 0 \\} = 1$. Let $F$ be their common distribution function. We seek a lower confidence bound for their common expectation $\\mu \\equiv \\mathbb E X_1 = \\int_0^\\infty x dF$. \n",
    "\n",
    "Under the hypothesis that $\\mu = t$, \n",
    "\n",
    "$$\n",
    "   t^{-1} \\mu =  t^{-1} \\int_0^\\infty  xdF(x) = \\int_0^\\infty xt^{-1} dF(x) = 1.\n",
    "$$\n",
    "Fix $\\gamma \\in [0, 1]$.\n",
    "Because $\\int_0^\\infty dF = 1$, it follows that if $\\mu  = t$,\n",
    "\n",
    "$$  \n",
    "    \\mathbb E \\left ( (\\gamma/t) X_j + (1-\\gamma) \\right ) = (\\gamma/t) \\mathbb E X_j + (1-\\gamma) =  \n",
    "     (\\gamma/t)t + (1-\\gamma) = 1.\n",
    "$$\n",
    "Now,\n",
    "$$\n",
    "   \\mathbb E \\left ((\\gamma/t) X_j + (1-\\gamma) \\right ) \\equiv \n",
    "     \\int_0^\\infty \\left (x \\gamma/t + (1-\\gamma) \\right ) dF(x).\n",
    "$$\n",
    "Since for $x \\ge 0$, $(x \\gamma/t + (1-\\gamma)) \\ge 0$, it follows that if we define\n",
    "\n",
    "$$\n",
    "   dG \\equiv (x \\gamma/t + (1-\\gamma))dF\n",
    "$$\n",
    "\n",
    "then $G$ is the cdf of a nonnegative random variable.\n",
    "\n",
    "Let $Y$ be a random variable with cdf $G$.\n",
    "By Jensen's inequality, $\\mathbb E X_j^2 \\ge (\\mathbb E X_j)^2 = t \\cdot \\mathbb E X_j$ (by hypothesis).\n",
    "Since $\\mathbb E X_j = t \\ge 0$,\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbb E Y  &= \\int_0^\\infty x dG(x) \\\\\n",
    "                 &= \\int_0^\\infty x (x \\gamma/t + (1-\\gamma)) dF(x) \\\\\n",
    "                 &= \\gamma/t \\int_0^\\infty x^2 dF(x) + (1-\\gamma) \\int_0^\\infty x dF(x) \\\\\n",
    "                 &= \\gamma/t \\cdot \\mathbb E X_j^2 + (1-\\gamma) \\cdot \\mathbb E X_j \\\\\n",
    "                 &\\ge \\gamma \\cdot \\mathbb E X_j + (1-\\gamma) \\cdot \\mathbb E X_j  = \\mathbb E X_j.\n",
    "\\end{align}\n",
    "(The penultimate step uses Jensen's inequality.)\n",
    "\n",
    "If the data allow us to reject the hypothesis $H_0$ that $\\{ X_j\\}$ are IID with cdf $F$\n",
    "in favor of the alternative hypothesis $H_1$ that they are iid with cdf $G$,\n",
    "we have strong statistical evidence that $\\mu = \\mathbb E X_j > t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the SPRT to test $H_1$ versus $H_0$\n",
    "\n",
    "Now a bit of magic occurs.  For a given observation $X_j = x_j$, what is the likelihood ratio of $H_1$ to $H_0$?\n",
    "\n",
    "$$\n",
    "    \\mbox{LR} = \\frac{dG(x_j)}{dF(x_j)} = \\frac{(x_j\\gamma/t+(1−\\gamma))dF(x_j)}{dF(x_j)} = (x_j\\gamma/t+(1−\\gamma)).\n",
    "$$\n",
    "This doesn't depend on $F$ or $G$!\n",
    "\n",
    "The $\\mbox{LR}$ for observations $X_1, \\ldots, X_m$ is\n",
    "$$\n",
    "      \\mbox{LR} = \\prod_{i=1}^m \\left [ (\\gamma/t) X_i + 1 - \\gamma \\right ].\n",
    "$$\n",
    "This expression shows why $\\gamma$ was introduced in the first place:\n",
    "for $\\gamma = 1$, if even a single observation turned out to be zero, $\\mbox{LR}$ would forever be\n",
    "zero no matter how large subsequent observations turned out to be.\n",
    "Taking $\\gamma < 1$ hedges against that possibility.\n",
    "Any value of $\\gamma \\in [0, 1]$ gives a conservative test, but smaller values provide more \"protection\"\n",
    "against small values of $X_j$ (but incur a loss of power when all $X_j$ are large).\n",
    "\n",
    "Recall that the $\\mbox{LR}$ is the $P$-value of $H_0: \\mu = t$ based on the observations $\\{X_j\\}_{j=1}^m$.\n",
    "We will use this to find a lower confidence bound for $\\mu$.\n",
    "\n",
    "### \"Lookahead\" and the SPRT\n",
    "There's more: recall that the SPRT says the chance that $\\mbox{LR}$ _ever_ gets larger than $1/\\alpha$ is at most $\\alpha$ if $H_0$ is true.\n",
    "That means that we can use the observations sequentially, maximizing over the partial products.\n",
    "If any of the partial products exceeds $1/\\alpha$, we can reject $H_0$.\n",
    "\n",
    "That is, our level-$\\alpha$ test based on a sample of size $n$ is\n",
    "$$\n",
    "    \\mbox{ Reject } H_0 \\mbox{ if } \\max_{m=1}^n \\prod_{i=1}^m \\left [ \\gamma X_i/t + 1 - \\gamma \\right ] \\ge 1/\\alpha.\n",
    "$$\n",
    "\n",
    "It is _only_ legitimate to do this maximization if the data are in random order.  For instance, it's impermissible to sort them from largest to smallest. And if you maximize over partial products, the result will, in general, depend on the order of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence bounds from the Kaplan-Wald test\n",
    "\n",
    "To find a lower confidence bound, we can invert hypothesis tests: the lower endpoint of a one-sided confidence bound for $\\mu$ is the largest value \n",
    "of $t$ for which we would not reject the hypothesis $\\mu = t$ at significance level $\\alpha$.\n",
    "\n",
    "For confidence levels above 50%, this lower confidence bound will certainly be between zero and the sample mean. However, for $t=0$, we have a divide-by-zero issue. Hence, for numerical implementation, we search the interval $[\\epsilon, \\bar{X}]$ for the smallest $t$ for which we can reject the hypothesis $\\mu = t$ at significance level $\\alpha$. If that smallest $t$ is $\\epsilon$, we set the confidence bound to zero. \n",
    "\n",
    "The following code implements this idea, working with the log of the test statistic to improve numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the first cell with code: set up the Python environment\n",
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy.polynomial import polynomial as P\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "from scipy.stats import binom\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaplanWaldLowerCI(x, cl = 0.95, gamma = 0.99, xtol=1.e-12, logf=True):\n",
    "    '''\n",
    "       Calculates the Kaplan-Wald lower 1-alpha confidence bound for the mean of a nonnegative random\n",
    "       variable.\n",
    "    '''\n",
    "    alpha = 1.0-cl\n",
    "    if any(x < 0):\n",
    "        raise ValueError('Data x must be nonnegative.')\n",
    "    elif all(x <= 0):\n",
    "        lo = 0.0\n",
    "    else:\n",
    "        if logf:\n",
    "            f = lambda t: (np.max(np.cumsum(np.log(gamma*x/t + 1 - gamma))) + np.log(alpha))\n",
    "        else:\n",
    "            f = lambda t: (np.max(np.cumprod(gamma*x/t + 1 - gamma)) - 1/alpha)\n",
    "        xm = np.mean(x)\n",
    "        if f(xtol)*f(xm) > 0.0:\n",
    "            lo = 0.0\n",
    "        else:\n",
    "            lo = sp.optimize.brentq(f, xtol, np.mean(x), xtol=xtol) \n",
    "    return lo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How well does it work?\n",
    "Let's test the method on our standard test problems: binomial and a mixture of pointmass and uniform.  We will fix $\\gamma$; you might experiment using different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Simulated coverage probability and expected lower endpoint of one-sided Student-t and Kaplan-Wald confidence intervals for mixture of U[0,1] and pointmass at 1 population</h3><strong>Nominal coverage probability 95.0%</strong>. Gamma=0.99.<br /><strong>Estimated from 10000 replications.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mass at 1</th>\n",
       "      <th>sample size</th>\n",
       "      <th>Student-t cov</th>\n",
       "      <th>Kaplan-Wald cov</th>\n",
       "      <th>Student-t low</th>\n",
       "      <th>Kaplan-Wald low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.900</td>\n",
       "      <td>25</td>\n",
       "      <td>89.57%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.6856</td>\n",
       "      <td>0.8201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.900</td>\n",
       "      <td>50</td>\n",
       "      <td>98.52%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.6727</td>\n",
       "      <td>0.8715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.900</td>\n",
       "      <td>100</td>\n",
       "      <td>99.97%</td>\n",
       "      <td>97.42%</td>\n",
       "      <td>0.6657</td>\n",
       "      <td>0.8969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.900</td>\n",
       "      <td>400</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>96.73%</td>\n",
       "      <td>0.6623</td>\n",
       "      <td>0.8987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.990</td>\n",
       "      <td>25</td>\n",
       "      <td>22.02%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9542</td>\n",
       "      <td>0.8792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.990</td>\n",
       "      <td>50</td>\n",
       "      <td>38.46%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9424</td>\n",
       "      <td>0.9341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.990</td>\n",
       "      <td>100</td>\n",
       "      <td>62.62%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.9626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.990</td>\n",
       "      <td>400</td>\n",
       "      <td>97.94%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9069</td>\n",
       "      <td>0.9849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.999</td>\n",
       "      <td>25</td>\n",
       "      <td>2.64%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9953</td>\n",
       "      <td>0.8854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.999</td>\n",
       "      <td>50</td>\n",
       "      <td>4.76%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9939</td>\n",
       "      <td>0.9406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.999</td>\n",
       "      <td>100</td>\n",
       "      <td>9.48%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9914</td>\n",
       "      <td>0.9694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.999</td>\n",
       "      <td>400</td>\n",
       "      <td>33.32%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9843</td>\n",
       "      <td>0.9917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mass at 1 sample size Student-t cov Kaplan-Wald cov Student-t low  \\\n",
       "0       0.900          25        89.57%          100.0%        0.6856   \n",
       "1       0.900          50        98.52%          100.0%        0.6727   \n",
       "2       0.900         100        99.97%          97.42%        0.6657   \n",
       "3       0.900         400        100.0%          96.73%        0.6623   \n",
       "4       0.990          25        22.02%          100.0%        0.9542   \n",
       "5       0.990          50        38.46%          100.0%        0.9424   \n",
       "6       0.990         100        62.62%          100.0%         0.927   \n",
       "7       0.990         400        97.94%          100.0%        0.9069   \n",
       "8       0.999          25         2.64%          100.0%        0.9953   \n",
       "9       0.999          50         4.76%          100.0%        0.9939   \n",
       "10      0.999         100         9.48%          100.0%        0.9914   \n",
       "11      0.999         400        33.32%          100.0%        0.9843   \n",
       "\n",
       "   Kaplan-Wald low  \n",
       "0           0.8201  \n",
       "1           0.8715  \n",
       "2           0.8969  \n",
       "3           0.8987  \n",
       "4           0.8792  \n",
       "5           0.9341  \n",
       "6           0.9626  \n",
       "7           0.9849  \n",
       "8           0.8854  \n",
       "9           0.9406  \n",
       "10          0.9694  \n",
       "11          0.9917  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nonstandard mixture: a pointmass at 1 and a uniform[0,1]\n",
    "ns = np.array([25, 50, 100, 400])  # sample sizes\n",
    "ps = np.array([0.9, 0.99, 0.999])    # mixture fraction, weight of pointmass\n",
    "alpha = 0.05  # 1- (confidence level)\n",
    "reps = int(10**4)   # just for demonstration\n",
    "gamma = 0.99\n",
    "xtol = 1.e-12\n",
    "\n",
    "simTable = pd.DataFrame(columns=('mass at 1', 'sample size', 'Student-t cov',\\\n",
    "                                 'Kaplan-Wald cov', 'Student-t low', 'Kaplan-Wald low')\n",
    "                       )\n",
    "\n",
    "for p in ps:\n",
    "    popMean = (1-p)*0.5 + p  # population is a mixture of uniform with mass (1-p) and\n",
    "                             # pointmass at 1 with mass p\n",
    "    \n",
    "    for n in ns:\n",
    "        tCrit = sp.stats.t.ppf(q=1-alpha, df=n-1)\n",
    "        coverT = 0\n",
    "        coverK = 0\n",
    "        lowT = 0.0\n",
    "        lowK = 0.0\n",
    "        \n",
    "        for rep in range(int(reps)):\n",
    "            sam = np.random.uniform(size=n)  # the uniform part of the sample\n",
    "            ptMass = np.random.uniform(size=n)  # for a bunch of p-coin tosses\n",
    "            sam[ptMass < p] = 1.0   # mix in pointmass at 1, with probability p\n",
    "            # t interval\n",
    "            samMean = np.mean(sam)\n",
    "            samSD = np.std(sam, ddof=1)\n",
    "            tLo = samMean - tCrit*samSD  # lower endpoint of the t interval\n",
    "            coverT += ( tLo <= popMean )\n",
    "            lowT += tLo\n",
    "            #  Kaplan-Wald interval\n",
    "            kLo = kaplanWaldLowerCI(sam, cl=1-alpha, gamma=gamma, xtol=xtol) # lower endpoint of the Kaplan-Wald interval\n",
    "            coverK += (kLo <= popMean )\n",
    "            lowK += kLo\n",
    "\n",
    "        simTable.loc[len(simTable)] =  p, n,\\\n",
    "            str(100*float(coverT)/float(reps)) + '%',\\\n",
    "            str(100*float(coverK)/float(reps)) + '%',\\\n",
    "            str(round(lowT/float(reps),4)),\\\n",
    "            str(round(lowK/float(reps),4))\n",
    "#\n",
    "ansStr =  '<h3>Simulated coverage probability and expected lower endpoint of ' +\\\n",
    "          'one-sided Student-t and Kaplan-Wald confidence intervals for ' +\\\n",
    "          'mixture of U[0,1] and pointmass at 1 population</h3>' +\\\n",
    "          '<strong>Nominal coverage probability ' + str(100*(1-alpha)) +\\\n",
    "          '%</strong>. Gamma=' + str(gamma) +\\\n",
    "          '.<br /><strong>Estimated from ' + str(int(reps)) +\\\n",
    "          ' replications.</strong>'\n",
    "\n",
    "display(HTML(ansStr))\n",
    "display(simTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good!  Next we will introduce one more method, then we'll compare the various methods we've seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper confidence bounds and two-sided bounds\n",
    "\n",
    "If every $X_j$ has the same finite, a priori upper bound $u$, we can transform the problem by defining $\\tilde{X_j}= u - X_j$.  Then $\\tilde{X_j}$ is a nonnegative random variable, and a lower confidence bound on its mean translated to can be subtracted from $u$ to make an upper confidence bound on $\\mathbb E X_j$.\n",
    "\n",
    "If every $X_j$ has the finite, a priori upper and lower bound, we can find two-sided confidence intervals in the analogous way, applying the Kaplan Wald method to the original data to find lower confidence bounds and to the data subtracted from the a priori upper bound to find upper confidence bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower confidence bounds on the mean for sampling without replacement: Kolmogorov's inequality\n",
    "\n",
    "This is an alternative derivation of the Kaplan-Wald method, also from [Harold Kaplan's website](http://web.archive.org/web/20131209044835/http://printmacroj.com/martMean.htm), involving\n",
    "the martingale property of a suitably constructed sequence.\n",
    "It relies on Kolmogorov's inequality for optionally stopped closed martingales.\n",
    "\n",
    "Suppose we are sampling without replacement from a finite population of $N$ \n",
    "non-negative items,  $\\{x_1, \\ldots, x_N\\}$, with $x_j \\ge 0$ $\\forall j$.\n",
    "The population mean is $\\mu = \\frac{1}{N} \\sum_{j=1}^N x_j \\ge 0$ and the population total\n",
    "is $N\\mu \\ge 0$.\n",
    "We draw $\\{X_1, \\ldots, X_n \\}$ sequentially without replacement.\n",
    "On the hypothesis $\\mu = t$, $\\mathbb{E}X_1 = t$, so $\\mathbb{E}(X_1/t) = 1$.\n",
    "Conditional on $X_1, \\ldots, X_n$, the total of the remaining $N-n$ items is\n",
    "$N\\mu - \\sum_{j=1}^n X_j$, so the mean of the remaining items is\n",
    "\n",
    "$$\n",
    "    \\frac{Nt-\\sum_{j=1}^n X_j}{N-n} = \\frac{t - \\frac{1}{N} \\sum_{j=1}^n X_j}{1-n/N}.\n",
    "$$\n",
    "Thus, the expected value of $X_{n+1}$ given $X_1, \\ldots, X_n$ is \n",
    "$\\frac{t - \\frac{1}{N} \\sum_{j=1}^n X_j}{1-n/N}$.\n",
    "\n",
    "Define \n",
    "\n",
    "$$\n",
    "Y_1(t) \\equiv\n",
    "\\begin{cases}\n",
    "X_1/t,& Nt > 0, \\\\\n",
    "1,&   Nt = 0, \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and for $1 \\le n \\le N-1$,\n",
    "$$\n",
    "Y_{n+1}(t) \n",
    "\\equiv\n",
    "\\begin{cases}\n",
    "X_{n+1} \n",
    "\\frac\n",
    "{1 - \\frac{n}{N}}\n",
    "{t - \\frac{1}{N} \\sum_{j=1}^n X_j},& \\sum_{j=1}^n X_j < Nt, \\\\\n",
    "1,& \\sum_{j=1}^n X_j \\ge Nt. \\\\\n",
    "\\end{cases}\n",
    "$$ \n",
    "\n",
    "Then $\\mathbb{E}(Y_{n+1}(t) | Y_1, \\ldots Y_n) = 1$.\n",
    "Let $Z_{n}(t) \\equiv \\prod_{j=1}^n Y_j(t)$.\n",
    "Note that $Y_k(t)$ can be recovered from $\\{Z_j(t), j \\le k\\}$, \n",
    "since $Y_k(t) = Z_k(t)/Z_{k-1}(t)$.\n",
    "Now $\\mathbb{E}|Z_k| \\le \\max_j x_j < \\infty$ and\n",
    "\n",
    "$$\n",
    "   \\mathbb{E}\\left ( Z_{n+1}(t) | Z_1(t), \\ldots Z_n(t) \\right ) = \n",
    "   \\mathbb{E} \\left (Y_{n+1}(t)Z_n(t) | Z_1(t), \\ldots Z_n(t) \\right ) = Z_n(t).\n",
    "$$\n",
    "\n",
    "Thus \n",
    "\n",
    "$$\n",
    "    \\left ( Z_1(t), Z_2(t), \\;\\ldots , Z_N(t) \\right )\n",
    "$$\n",
    "\n",
    "is a non-negative closed martingale.\n",
    "\n",
    "By Kolmogorov's inequality, an application of Markov's inequality to martingales\n",
    "(Feller V2, p.242), for any $p > 0$ and any $J \\in \\{1, \\ldots, N \\}$,\n",
    "\n",
    "$$\n",
    "     \\Pr \\left ( \\max_{1 \\le j \\le J} Z_j(t) > 1/p \\right ) \\le p \\; \\mathbb{E}|Z_J|.\n",
    "$$\n",
    "\n",
    "Since $(Z_j)$ is a non-negative martingale, $\\mathbb{E}|Z_J| = \\mathbb{E}Z_J = \\mathbb{E}Z_1 = 1$.\n",
    "\n",
    "Thus a $p$-value for the hypothesis $\\mu = t$ based on data $X_1, \\ldots X_J$ is \n",
    "$\\left (\\max_{1 \\le j \\le J} Z_j(t) \\right )^{-1} \\wedge 1$.\n",
    "If $X_j = 0$ for some $j$, then $Z_k = 0$ for all $k \\ge j$.\n",
    "\n",
    "To avoid that problem, we can shift everything to the right: pick $\\gamma > 0$,\n",
    "find a lower confidence bound for $\\delta = \\mu+\\gamma > \\mu > 0$ from data $\\{X_j+\\gamma\\}$, then subtract $\\gamma$ from the lower \n",
    "confidence bound to get a lower confidence bound for $\\mu$.\n",
    "There are tradeoffs involved in picking $\\gamma$: if many $X_j$ turn out to be \n",
    "small, especially for small $j$, it helps to have $\\gamma$ large, and vice versa.\n",
    "\n",
    "Unpacking the math yields the $p$ value\n",
    "\n",
    "$$\n",
    "p_{\\mathrm{KK}} \\equiv 1 \\wedge \\left ( \\max_{1 \\le j \\le J} \\prod_{k=1}^j (X_{k}+\\gamma) \\frac{1-(k-1)/N}{t - \\frac{1}{N} \\sum_{\\ell=1}^{k-1} (X_\\ell+\\gamma)} \\right )^{-1}\n",
    "$$\n",
    "for the hypothesis that $\\mu \\le t - \\gamma$.\n",
    "\n",
    "The corresponding $1-\\alpha$ lower confidence bound is\n",
    "\n",
    "$$\n",
    "   \\sup \\left \\{t \\ge 0: \\max_{1 \\le j \\le J}  \\prod_{k=1}^j (X_{k}+\\gamma) \\frac{1-(k-1)/N}{t - \\frac{1}{N} \\sum_{\\ell=1}^{k-1} (X_\\ell+\\gamma)} \\le 1/\\alpha \\right \\} - \\gamma.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: marbles in a jar\n",
    "\n",
    "A certain jar contains $N$ (even) marbles, $G$ green and $N-G$ non-green.\n",
    "We sample marbles sequentially without replacement. We wish to test the hypothesis\n",
    "$G \\le N/2$ against the alternative.\n",
    "For instance, $G$ might represent the number of votes for the reported winner of an election\n",
    "in which $N$ votes were cast.\n",
    "If the social choice function requires a majority, or if there are only two candidates, then\n",
    "green wins if $G > N/2$ and the outcome is a tie or a loss if $G \\le N/2$.\n",
    "If we can reject the hypothesis that $G \\le N/2$, we can conclude that the outcome is correct.\n",
    "\n",
    "Use the previous method, taking $t = 1/2$ and shift $\\delta > 0$.\n",
    "A pleasing choice for symmetry would be $\\delta = 1/2$, but the operating characteristics\n",
    "depend on the true value of $G$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another martingale test for the mean for sampling without replacement\n",
    "\n",
    "Let's define a finite-sample version of the Kaplan-Wald approach, to which we can apply Kolmogorov's Inequality.\n",
    "\n",
    "Let $S_j \\equiv \\sum_{k=1}^j X_k$, $\\tilde{S}_j \\equiv S_j/N$, and $\\tilde{j} \\equiv 1 - (j-1)/N$.\n",
    "Define\n",
    "\n",
    "$$\n",
    "   Y_n \\equiv \\int_0^1 \\prod_{j=1}^n \\left ( \\gamma \\left [ X_j \\frac{\\tilde{j}}{t - \\tilde{S}_{j-1}} -1 \\right ] + 1 \\right ) d\\gamma.\n",
    "$$\n",
    "\n",
    "This is a polynomial in $\\gamma$ of degree at most $n$, with constant term $1$.\n",
    "Each $X_j$ appears linearly.\n",
    "Under the null hypothesis that the population total is $Nt$, $\\mathbb{E} X_1 = t$,\n",
    "and \n",
    "\n",
    "$$\n",
    "  \\mathbb{E} \\left ( X_j \\mid X_1, \\ldots, X_{j-1} \\right ) = \n",
    "  \\frac{Nt - S_{j-1}}{N-j+1} = \\frac{t - \\tilde{S}_{j-1}}{\\tilde{j}}.\n",
    "$$\n",
    "\n",
    "Now \n",
    "\n",
    "$$\n",
    "   Y_1 = \\int_0^1 \\left ( \\gamma [ X_1/t - 1] + 1 \\right ) d\\gamma = \n",
    "   \\left [ (\\gamma^2/2) [X_1/t - 1] + \\gamma \\right ]_{\\gamma=0}^1 = [X_1/t - 1]/2 + 1 = \\frac{X_1}{2t} + 1/2.\n",
    "$$\n",
    "\n",
    "Thus, under the null, \n",
    "$$\n",
    "   \\mathbb{E}Y_1 = \\frac{\\mathbb{E}X_1}{2t} + 1/2 = 1.\n",
    "$$\n",
    "\n",
    "Also,\n",
    "\n",
    "$$\n",
    "   \\mathbb{E}(Y_n | X_1, \\ldots, X_{n-1}) =\n",
    "   \\mathbb{E} \\left . \\left [ \\int_0^1 \\prod_{j=1}^n \\left (\\gamma \\left [ X_j \\frac{\\tilde{j}}{t - \\tilde{S}_{j-1}} -1 \\right ] + 1 \\right ) d\\gamma \\right | X_1, \\ldots, X_{n-1} \\right ]\n",
    "$$\n",
    "$$\n",
    "  = \\int_0^1  \\left (\\gamma \\left [ \\mathbb{E}(X_n | X_1, \\ldots, X_{n-1}) \\frac{\\tilde{n}}{t - \\tilde{S}_{n-1}} -1 \\right ] + 1 \\right ) \\prod_{j=1}^{n-1} \\left ( \\gamma \\left [ X_j \\frac{\\tilde{j}}{t - \\tilde{S}_{j-1}} -1 \\right ] + 1 \\right ) d\\gamma \n",
    "$$\n",
    "$$\n",
    "  = \\int_0^1  \\left (\\gamma \\left [ \\frac{t - \\tilde{S}_{n-1}}{\\tilde{n}} \\frac{\\tilde{n}}{t - \\tilde{S}_{n-1}} -1 \\right ] + 1 \\right ) \\prod_{j=1}^{n-1} \\left ( \\gamma \\left [ X_j \\frac{\\tilde{j}}{t - \\tilde{S}_{j-1}} -1 \\right ] + 1 \\right ) d\\gamma \n",
    "$$\n",
    "$$\n",
    "  = \\int_0^1  \\prod_{j=1}^{n-1} \\left ( \\gamma \\left [ X_j \\frac{\\tilde{j}}{t - \\tilde{S}_{j-1}} -1 \\right ] + 1 \\right ) d\\gamma = Y_{n-1}.\n",
    "$$\n",
    "\n",
    "Thus, under the null hypothesis, $(Y_j )_{j=1}^N$ is a nonnegative closed martingale with expected value 1, and Kolmogorov's inequality implies that for any $J \\in \\{1, \\ldots, N\\}$,\n",
    "\n",
    "$$\n",
    "   \\Pr \\left ( \\max_{1 \\le j \\le J} Y_j(t) > 1/p \\right ) \\le p.\n",
    "$$\n",
    "\n",
    "Set \n",
    "\n",
    "$$\n",
    "    c_j \\equiv X_j \\frac{\\tilde{j}}{t - \\tilde{S}_{j-1}} -1,\n",
    "$$\n",
    "\n",
    "and re-write the polynomial\n",
    "\n",
    "$$\n",
    " \\prod_{j=1}^n \\left (\\gamma \\left [ X_j \\frac{\\tilde{j}}{t - \\tilde{S}_{j-1}} -1 \\right ] + 1 \\right ) = \\prod_{j: c_j \\ne 0} c_j(\\gamma + c_j^{-1}).\n",
    "$$\n",
    "This expresses the polynomial in terms of its roots, facilitating computations in Python.\n",
    "Using this notation,\n",
    "\n",
    "$$\n",
    "   Y_j = \\int_0^1 \\prod_{j: c_j \\ne 0} c_j(\\gamma + c_j^{-1}) d\\gamma =\n",
    "   \\left ( \\prod_{j: c_j \\ne 0} c_j \\right ) \\int_0^1 \\prod_{j: c_j \\ne 0} (\\gamma + c_j^{-1}) d\\gamma.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KK_p(x, N, t, random_order = True):\n",
    "    '''\n",
    "    p-value for the hypothesis that the mean of a nonnegative population with N\n",
    "    elements is t. The alternative is that the mean is larger than t.\n",
    "    If the random sample x is in the order in which the sample was drawn, it is\n",
    "    legitimate to set random_order = True. \n",
    "    If not, set random_order = False. \n",
    "    '''\n",
    "    x = np.array(x)\n",
    "    assert all(x >=0),  'Negative value in a nonnegative population!'\n",
    "    assert len(x) <= N, 'Sample size is larger than the population!'\n",
    "    assert N > 0,       'Population size not positive!'\n",
    "    assert N == int(N), 'Non-integer population size!'\n",
    "    sample_total = 0.0\n",
    "    mart = x[0]/t if t > 0 else 1\n",
    "    mart_max = mart\n",
    "    for j in range(1, len(x)):\n",
    "        mart *= x[j]*(1-j/N)/(t - (1/N)*sample_total)\n",
    "        if mart < 0:\n",
    "            mart = np.inf\n",
    "            break\n",
    "        else:\n",
    "            sample_total += x[j]\n",
    "        mart_max = max(mart, mart_max)\n",
    "    p = min((1/mart_max if random_order else 1/mart),1)\n",
    "    return p   \n",
    "\n",
    "def HK_p(x, N, t, random_order = True):\n",
    "    '''\n",
    "    Harold Kaplan p-value for the hypothesis that the mean of a nonnegative \n",
    "    population with N elements is t. \n",
    "    The alternative is that the mean is larger than t.\n",
    "    If the random sample x is in the order in which the sample was drawn, it is\n",
    "    legitimate to set random_order = True. \n",
    "    If not, set random_order = False. \n",
    "    '''\n",
    "    x = np.array(x)\n",
    "    assert all(x >=0),  'Negative value in a nonnegative population!'\n",
    "    assert len(x) <= N, 'Sample size is larger than the population!'\n",
    "    assert N > 0,       'Population size not positive!'\n",
    "    assert N == int(N), 'Non-integer population size!'\n",
    "    Stilde = (np.insert(np.cumsum(x),0,0)/N)[0:len(x)] # \\tilde{S}_{j-1}\n",
    "    t_minus_Stilde = t - Stilde\n",
    "    mart_max = 1\n",
    "    if any(t_minus_Stilde < 0):  # sample total exceeds hypothesized population total\n",
    "        mart_max = np.inf\n",
    "    else:  # TO FIX: need to deal with zeros in t_minus_stilde\n",
    "        jtilde = 1 - np.array(list(range(len(x))))/N\n",
    "        # c_j = X_j*\\tilde{j}/(t-\\tilde{S}_{j-1}) - 1\n",
    "        c = np.multiply(x, np.divide(jtilde, t_minus_Stilde))-1  # need to test for zeros!\n",
    "        for j in range(len(x)):\n",
    "            r = -np.array([1/cc for cc in c[0:j+1] if cc != 0]) # roots\n",
    "            if r.size > 0:\n",
    "                Y_norm = np.prod(np.array([cc for cc in c[0:j+1] if cc != 0]))  # multiplicative constant\n",
    "                poly = P.polyfromroots(r)\n",
    "                poly_int = P.polyint(poly)\n",
    "                poly_int_values = P.polyval([1], poly_int)\n",
    "                mart = Y_norm*poly_int_values[0]\n",
    "                mart_max = max(mart_max, mart) if random_order else mart\n",
    "            else:\n",
    "                mart_max = max(mart_max, 1) if random_order else 1\n",
    "    p = min(1/mart_max,1)\n",
    "    return p   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Harold Kaplan's javascript routine asStated() translated into python\n",
    "\n",
    "def HK_js_p(x, N, t, random_order = True):\n",
    "    x = np.array(x)\n",
    "    assert all(x >=0),  'Negative value in a nonnegative population!'\n",
    "    assert len(x) <= N, 'Sample size is larger than the population!'\n",
    "    assert N > 0,       'Population size not positive!'\n",
    "    assert N == int(N), 'Non-integer population size!'\n",
    "    big = 0.\n",
    "    denominator = 1\n",
    "    sumOfPreviousXValues = 0.\n",
    "    n = len(x)\n",
    "    w = np.zeros(n+1)\n",
    "    w[0] = 1\n",
    "    for j in range(1,n+1):\n",
    "        reduced_t = t - sumOfPreviousXValues/N\n",
    "        if reduced_t < 0:\n",
    "            return 0\n",
    "        expectedX = reduced_t/( 1-(j-1)/N )\n",
    "        if expectedX > 0:\n",
    "            quotient = x[j-1]/expectedX\n",
    "            for k in range(j,0,-1):\n",
    "                w[k] = quotient*w[k-1]*k/j + w[k]*(j-k)/j\n",
    "            denominator += 1\n",
    "        tot=0\n",
    "        for k in range(j+1):\n",
    "            tot += w[k]\n",
    "        candidate = tot/denominator\n",
    "        big = max(big, candidate) if random_order else candidate\n",
    "        sumOfPreviousXValues += x[j-1]\n",
    "        reduced_t = t - sumOfPreviousXValues/N\n",
    "        if reduced_t < 0:\n",
    "            return 0\n",
    "    return min(1/big,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00013854893672071193 0.00013854893672071193\n"
     ]
    }
   ],
   "source": [
    "# Compare the python implementation with the translation of Kaplan's javascript into python\n",
    "x = np.array(range(10)) \n",
    "t = 1\n",
    "N = 1000\n",
    "print(HK_p(x, N, t), HK_js_p(x, N, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CI_from_test(x, N, cl=0.95, d = 0, random_order = True,\n",
    "                         p_value = KK_p, **kwargs):\n",
    "    '''\n",
    "    Lower confidence bound for the mean of a finite, nonnegative population.\n",
    "    \n",
    "    x: data, a random sample without replacement\n",
    "    N: population size\n",
    "    cl: desired confidence level\n",
    "    d: a shift in case x contains zeros. d must be selected before looking at the data\n",
    "    random_order: if True, the data x must be in the (random) order in which the sample was\n",
    "        drawn. If x is not known to be in random order, set random_order = False\n",
    "    '''\n",
    "    \n",
    "    assert cl > 0.5,    'Confidence level must be at least 50%.'\n",
    "    assert all(x >=0),  'Negative value in a nonnegative population!'\n",
    "    assert len(x) <= N, 'Sample size is larger than the population!'\n",
    "    assert N > 0,       'Population size not positive!'\n",
    "    assert N == int(N), 'Non-integer population size!'\n",
    "    \n",
    "    x = np.array(x)\n",
    "    \n",
    "    # a lower confidence bound at cl > 0.5 should not be larger than the sample mean\n",
    "    if random_order:\n",
    "        u = np.amax(np.array([np.mean(x[0:j+1]) for j in range(len(x))]))\n",
    "    else:\n",
    "        u = np.mean(x) \n",
    "    f = lambda t: p_value(x+d, N, t) - (1-cl)\n",
    "    lcl = 0.0\n",
    "    if f(0) < 0.0:\n",
    "        lcl = brentq(f, 0, u+d, *kwargs) - d\n",
    "    return max(lcl,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.5775009944552949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 5.466944669607846\n",
      "100 6.529685928682667\n",
      "250 7.001662183858742\n",
      "500 6.149003688929668\n",
      "750 4.900993817811354\n",
      "1000 3.5376362122801766\n"
     ]
    }
   ],
   "source": [
    "## compare performance for different values of delta\n",
    "\n",
    "cl = 1-.05\n",
    "\n",
    "N=200000               # population size\n",
    "x = [0]*450 + [100]*50 # sample size 500\n",
    "x = np.array(x)\n",
    "reps = int(10**3)\n",
    "delta = [1, 50, 100, 250, 500, 750, 1000]\n",
    "out = np.zeros(len(delta))\n",
    "for j in range(len(delta)):\n",
    "    sims = np.zeros(reps)\n",
    "    for i in range(reps):\n",
    "        xp = np.random.permutation(x)\n",
    "        sims[i] = CI_from_test(x, N, d=delta[j], cl=cl)\n",
    "    out[j] = np.mean(sims)\n",
    "    print(delta[j], out[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "f(a) and f(b) must have different signs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-f66d95fe61a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mxp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     sims[i] = CI_from_test(x, N, d=0, cl=cl, random_order = True, \n\u001b[0;32m---> 13\u001b[0;31m                                   p_value = HK_p)\n\u001b[0m\u001b[1;32m     14\u001b[0m     sims_js[i] = CI_from_test(x, N, d=0, cl=cl, random_order = True, \n\u001b[1;32m     15\u001b[0m                                   p_value = HK_js_p)\n",
      "\u001b[0;32m<ipython-input-30-f67a39012b33>\u001b[0m in \u001b[0;36mCI_from_test\u001b[0;34m(x, N, cl, d, random_order, p_value, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mlcl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mlcl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrentq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlcl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/scipy/optimize/zeros.py\u001b[0m in \u001b[0;36mbrentq\u001b[0;34m(f, a, b, args, xtol, rtol, maxiter, full_output, disp)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrtol\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0m_rtol\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rtol too small (%g < %g)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_zeros\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_brentq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: f(a) and f(b) must have different signs"
     ]
    }
   ],
   "source": [
    "cl = 1-.05\n",
    "\n",
    "N = 200000\n",
    "x = [0]*250 + [100]*250 \n",
    "x = np.array(x)\n",
    "reps = int(10**3)\n",
    "\n",
    "sims_js = np.zeros(reps)\n",
    "sims = np.zeros(reps)\n",
    "for i in range(reps):\n",
    "    xp = np.random.permutation(x)\n",
    "    sims[i] = CI_from_test(x, N, d=0, cl=cl, random_order = True, \n",
    "                                  p_value = HK_p)\n",
    "    sims_js[i] = CI_from_test(x, N, d=0, cl=cl, random_order = True, \n",
    "                                  p_value = HK_js_p)\n",
    "print(np.mean(sims), np.mean(sims_js))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "reps = 10**4\n",
    "p = np.zeros(reps)\n",
    "for i in range(int(reps)):\n",
    "    p[i] = HK_p(2*np.random.random(n), N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a161d91d0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOQW9iIjPKehFRHzufwFijbtBFVh6dQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pct = np.percentile(p, range(100))\n",
    "plt.plot(pct, range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:23: RuntimeWarning: overflow encountered in double_scalars\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Simulated coverage probability and expected lower endpoint of one-sided Kaplan-Kolmogorov and Kaplan-Wald confidence intervals for mixture of U[0,1] and pointmass at 1 population of20000items</h3><strong>Nominal coverage probability 95.0%</strong>. d=1.<br /><strong>Estimated from 1000 replications.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mass at 1</th>\n",
       "      <th>sample size</th>\n",
       "      <th>Kaplan-Kolmogorov cov</th>\n",
       "      <th>Kaplan-Wald cov</th>\n",
       "      <th>KK low</th>\n",
       "      <th>Kaplan-Wald low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.900</td>\n",
       "      <td>25</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.7798</td>\n",
       "      <td>0.7238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.900</td>\n",
       "      <td>50</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.8517</td>\n",
       "      <td>0.8298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.900</td>\n",
       "      <td>100</td>\n",
       "      <td>99.9%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.8936</td>\n",
       "      <td>0.8871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.900</td>\n",
       "      <td>400</td>\n",
       "      <td>98.0%</td>\n",
       "      <td>98.0%</td>\n",
       "      <td>0.9261</td>\n",
       "      <td>0.9288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.990</td>\n",
       "      <td>25</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.8313</td>\n",
       "      <td>0.7692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.990</td>\n",
       "      <td>50</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9089</td>\n",
       "      <td>0.8791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.990</td>\n",
       "      <td>100</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9496</td>\n",
       "      <td>0.9356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.990</td>\n",
       "      <td>400</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>0.9795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.999</td>\n",
       "      <td>25</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.8367</td>\n",
       "      <td>0.7738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.999</td>\n",
       "      <td>50</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9136</td>\n",
       "      <td>0.8831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.999</td>\n",
       "      <td>100</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9556</td>\n",
       "      <td>0.9406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.999</td>\n",
       "      <td>400</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9882</td>\n",
       "      <td>0.9846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mass at 1 sample size Kaplan-Kolmogorov cov Kaplan-Wald cov  KK low  \\\n",
       "0       0.900          25                100.0%          100.0%  0.7798   \n",
       "1       0.900          50                100.0%          100.0%  0.8517   \n",
       "2       0.900         100                 99.9%          100.0%  0.8936   \n",
       "3       0.900         400                 98.0%           98.0%  0.9261   \n",
       "4       0.990          25                100.0%          100.0%  0.8313   \n",
       "5       0.990          50                100.0%          100.0%  0.9089   \n",
       "6       0.990         100                100.0%          100.0%  0.9496   \n",
       "7       0.990         400                100.0%          100.0%  0.9821   \n",
       "8       0.999          25                100.0%          100.0%  0.8367   \n",
       "9       0.999          50                100.0%          100.0%  0.9136   \n",
       "10      0.999         100                100.0%          100.0%  0.9556   \n",
       "11      0.999         400                100.0%          100.0%  0.9882   \n",
       "\n",
       "   Kaplan-Wald low  \n",
       "0           0.7238  \n",
       "1           0.8298  \n",
       "2           0.8871  \n",
       "3           0.9288  \n",
       "4           0.7692  \n",
       "5           0.8791  \n",
       "6           0.9356  \n",
       "7           0.9795  \n",
       "8           0.7738  \n",
       "9           0.8831  \n",
       "10          0.9406  \n",
       "11          0.9846  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nonstandard mixture: a pointmass at 1 and a uniform[0,1]\n",
    "ns = np.array([25, 50, 100, 400])  # sample sizes\n",
    "ps = np.array([0.9, 0.99, 0.999])    # mixture fraction, weight of pointmass\n",
    "alpha = 0.05  # 1- (confidence level)\n",
    "reps = int(10**3)   # just for demonstration\n",
    "d = 1\n",
    "xtol = 1.e-12\n",
    "N = 20000 # hypothetical population size\n",
    "\n",
    "simTable = pd.DataFrame(columns=('mass at 1', 'sample size', 'Kaplan-Kolmogorov cov',\\\n",
    "                                 'Kaplan-Wald cov', 'KK low', 'Kaplan-Wald low')\n",
    "                       )\n",
    "\n",
    "for p in ps:\n",
    "    popMean = (1-p)*0.5 + p  # population is a mixture of uniform with mass (1-p) and\n",
    "                             # pointmass at 1 with mass p\n",
    "    \n",
    "    for n in ns:\n",
    "        tCrit = sp.stats.t.ppf(q=1-alpha, df=n-1)\n",
    "        coverT = 0\n",
    "        coverK = 0\n",
    "        lowT = 0.0\n",
    "        lowK = 0.0\n",
    "        \n",
    "        for rep in range(int(reps)):\n",
    "            sam = np.random.uniform(size=n)  # the uniform part of the sample\n",
    "            ptMass = np.random.uniform(size=n)  # for a bunch of p-coin tosses\n",
    "            sam[ptMass < p] = 1.0   # mix in pointmass at 1, with probability p\n",
    "            # Kaplan's mixture\n",
    "            tLo = CI_from_test(sam, N, d=0, cl=cl, random_order = True, \n",
    "                                  p_value = HK_p)\n",
    "            coverT += ( tLo <= popMean )\n",
    "            lowT += tLo\n",
    "            #  Kaplan-Wald interval\n",
    "            kLo = CI_from_test(sam, N, d=d, cl=cl, random_order = True, \n",
    "                                  p_value = KK_p) # lower endpoint of the Kaplan-Wald interval\n",
    "            coverK += (kLo <= popMean )\n",
    "            lowK += kLo\n",
    "\n",
    "        simTable.loc[len(simTable)] =  p, n,\\\n",
    "            str(100*float(coverT)/float(reps)) + '%',\\\n",
    "            str(100*float(coverK)/float(reps)) + '%',\\\n",
    "            str(round(lowT/float(reps),4)),\\\n",
    "            str(round(lowK/float(reps),4))\n",
    "#\n",
    "ansStr =  '<h3>Simulated coverage probability and expected lower endpoint of ' +\\\n",
    "          'one-sided Kaplan-Kolmogorov and Kaplan-Wald confidence intervals for ' +\\\n",
    "          'mixture of U[0,1] and pointmass at 1 population of' + str(N) + 'items</h3>' +\\\n",
    "          '<strong>Nominal coverage probability ' + str(100*(1-alpha)) +\\\n",
    "          '%</strong>. d=' + str(d) +\\\n",
    "          '.<br /><strong>Estimated from ' + str(int(reps)) +\\\n",
    "          ' replications.</strong>'\n",
    "\n",
    "display(HTML(ansStr))\n",
    "display(simTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Simulated coverage probability and expected lower endpoint of one-sided Kaplan-Kolmogorov and Kaplan-Wald confidence intervals for mixture of U[0,1] and pointmass at 0 population of20000items</h3><strong>Nominal coverage probability 95.0%</strong>. d=1.<br /><strong>Estimated from 1000 replications.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mass at 0</th>\n",
       "      <th>sample size</th>\n",
       "      <th>Kaplan-Kolmogorov cov</th>\n",
       "      <th>Kaplan-Wald cov</th>\n",
       "      <th>KK low</th>\n",
       "      <th>KW low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.900</td>\n",
       "      <td>25</td>\n",
       "      <td>98.3%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.900</td>\n",
       "      <td>50</td>\n",
       "      <td>98.5%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.0119</td>\n",
       "      <td>0.0013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.900</td>\n",
       "      <td>100</td>\n",
       "      <td>97.4%</td>\n",
       "      <td>99.0%</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.900</td>\n",
       "      <td>400</td>\n",
       "      <td>97.5%</td>\n",
       "      <td>96.8%</td>\n",
       "      <td>0.0288</td>\n",
       "      <td>0.0344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.990</td>\n",
       "      <td>25</td>\n",
       "      <td>98.1%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.990</td>\n",
       "      <td>50</td>\n",
       "      <td>98.7%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.990</td>\n",
       "      <td>100</td>\n",
       "      <td>98.9%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.990</td>\n",
       "      <td>400</td>\n",
       "      <td>98.5%</td>\n",
       "      <td>99.9%</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.999</td>\n",
       "      <td>25</td>\n",
       "      <td>99.4%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.999</td>\n",
       "      <td>50</td>\n",
       "      <td>99.5%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.999</td>\n",
       "      <td>100</td>\n",
       "      <td>99.3%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.999</td>\n",
       "      <td>400</td>\n",
       "      <td>99.1%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mass at 0 sample size Kaplan-Kolmogorov cov Kaplan-Wald cov  KK low  \\\n",
       "0       0.900          25                 98.3%          100.0%  0.0086   \n",
       "1       0.900          50                 98.5%          100.0%  0.0119   \n",
       "2       0.900         100                 97.4%           99.0%  0.0178   \n",
       "3       0.900         400                 97.5%           96.8%  0.0288   \n",
       "4       0.990          25                 98.1%          100.0%  0.0003   \n",
       "5       0.990          50                 98.7%          100.0%  0.0003   \n",
       "6       0.990         100                 98.9%          100.0%  0.0003   \n",
       "7       0.990         400                 98.5%           99.9%  0.0008   \n",
       "8       0.999          25                 99.4%          100.0%     0.0   \n",
       "9       0.999          50                 99.5%          100.0%     0.0   \n",
       "10      0.999         100                 99.3%          100.0%     0.0   \n",
       "11      0.999         400                 99.1%          100.0%     0.0   \n",
       "\n",
       "    KW low  \n",
       "0   0.0001  \n",
       "1   0.0013  \n",
       "2    0.012  \n",
       "3   0.0344  \n",
       "4      0.0  \n",
       "5      0.0  \n",
       "6      0.0  \n",
       "7   0.0001  \n",
       "8      0.0  \n",
       "9      0.0  \n",
       "10     0.0  \n",
       "11     0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nonstandard mixture: a pointmass at 0 and a uniform[0,1]\n",
    "ns = np.array([25, 50, 100, 400])  # sample sizes\n",
    "ps = np.array([0.9, 0.99, 0.999])    # mixture fraction, weight of pointmass\n",
    "alpha = 0.05  # 1- (confidence level)\n",
    "reps = int(10**3)   # just for demonstration\n",
    "d = 1\n",
    "xtol = 1.e-12\n",
    "N = 20000 # hypothetical population size\n",
    "\n",
    "simTable = pd.DataFrame(columns=('mass at 0', 'sample size', 'Kaplan-Kolmogorov cov',\\\n",
    "                                 'Kaplan-Wald cov', 'KK low', 'KW low')\n",
    "                       )\n",
    "\n",
    "for p in ps:\n",
    "    popMean = (1-p)*0.5 + p*0  # population is a mixture of uniform with mass (1-p) and\n",
    "                               # pointmass at 0 with mass p\n",
    "    \n",
    "    for n in ns:\n",
    "        tCrit = sp.stats.t.ppf(q=1-alpha, df=n-1)\n",
    "        coverT = 0\n",
    "        coverK = 0\n",
    "        lowT = 0.0\n",
    "        lowK = 0.0\n",
    "        \n",
    "        for rep in range(int(reps)):\n",
    "            sam = np.random.uniform(size=n)  # the uniform part of the sample\n",
    "            ptMass = np.random.uniform(size=n)  # for a bunch of p-coin tosses\n",
    "            sam[ptMass < p] = 0.0   # mix in pointmass at 0, with probability p\n",
    "            # Kaplan's mixture\n",
    "            tLo = CI_from_test(sam, N, d=0, cl=cl, random_order = True, \n",
    "                                  p_value = HK_p)\n",
    "            coverT += ( tLo <= popMean )\n",
    "            lowT += tLo\n",
    "            #  Kaplan-Wald interval\n",
    "            kLo = CI_from_test(sam, N, d=d, cl=cl, random_order = True, \n",
    "                                  p_value = KK_p) # lower endpoint of the Kaplan-Wald interval\n",
    "            coverK += (kLo <= popMean )\n",
    "            lowK += kLo\n",
    "\n",
    "        simTable.loc[len(simTable)] =  p, n,\\\n",
    "            str(100*float(coverT)/float(reps)) + '%',\\\n",
    "            str(100*float(coverK)/float(reps)) + '%',\\\n",
    "            str(round(lowT/float(reps),4)),\\\n",
    "            str(round(lowK/float(reps),4))\n",
    "#\n",
    "ansStr =  '<h3>Simulated coverage probability and expected lower endpoint of ' +\\\n",
    "          'one-sided Kaplan-Kolmogorov and Kaplan-Wald confidence intervals for ' +\\\n",
    "          'mixture of U[0,1] and pointmass at 0 population of' + str(N) + 'items</h3>' +\\\n",
    "          '<strong>Nominal coverage probability ' + str(100*(1-alpha)) +\\\n",
    "          '%</strong>. d=' + str(d) +\\\n",
    "          '.<br /><strong>Estimated from ' + str(int(reps)) +\\\n",
    "          ' replications.</strong>'\n",
    "\n",
    "display(HTML(ansStr))\n",
    "display(simTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "+ [Next: Dollar-unit sampling and taint](dus.ipynb)\n",
    "+ [Previous: Wald's Sequential Probability Ratio Test](sprt.ipynb)\n",
    "+ [Index](index.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
